% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiLLMviaionet.R
\name{multiLLMviaionet}
\alias{multiLLMviaionet}
\title{multiLLMviaionet: Execute ALL available LLM models simultaneously via io.net API}
\usage{
multiLLMviaionet(
  prompt,
  max_models = NULL,
  streaming = FALSE,
  random_selection = FALSE,
  api_key = Sys.getenv("IONET_API_KEY"),
  max_tokens = 1024,
  temperature = 0.7,
  timeout = 300,
  parallel = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{prompt}{A string containing the input prompt to send to all selected models.}

\item{max_models}{Integer specifying maximum number of models to run (1-50). Default is all available models.}

\item{streaming}{Logical indicating whether to use streaming responses. Default is FALSE.}

\item{random_selection}{Logical indicating whether to randomly select models from all available models. 
If TRUE, randomly selects up to max_models from all available models.
If FALSE, uses all available models in order (up to max_models limit). Default is FALSE.}

\item{api_key}{A string containing the io.net API key. 
Defaults to the environment variable "IONET_API_KEY".}

\item{max_tokens}{Integer specifying maximum tokens to generate per model. Default is 1024.}

\item{temperature}{Numeric value controlling randomness (0-2). Default is 0.7.}

\item{timeout}{Numeric value in seconds for request timeout per model. Default is 300.}

\item{parallel}{Logical indicating whether to run models in parallel. Default is TRUE.}

\item{verbose}{Logical indicating whether to show detailed progress. Default is TRUE.}
}
\value{
A list containing:
  \describe{
    \item{results}{List of responses from each model with metadata}
    \item{summary}{Summary statistics including execution times and token usage}
    \item{errors}{Any errors encountered during execution}
    \item{models_used}{Character vector of models that were actually executed}
  }
}
\description{
This function automatically runs the same prompt across ALL currently available 
  LLM models on the io.net API. It supports running all models in parallel with streaming 
  responses and comprehensive error handling.
  
  The function dynamically fetches the current list of available models from the io.net API
  and executes ALL of them by default (unless limited by max_models parameter). Results are 
  cached for 1 hour to improve performance. If the API is unavailable, it falls back to a static model list.
  
  Supported model categories include:
  - Meta Llama series (Llama-4-Maverick, Llama-3.3-70B, etc.)
  - DeepSeek series (DeepSeek-R1, DeepSeek-R1-Distill variants)
  - Qwen series (Qwen3-235B, QwQ-32B, Qwen2.5-Coder, etc.)
  - Specialized models (AceMath-7B, watt-tool-70B, ReaderLM-v2)
  - Microsoft Phi, Mistral, Google Gemma, and more
}
\details{
Multi-LLM via io.net API
}
\examples{
\dontrun{
  # Set your io.net API key
  Sys.setenv(IONET_API_KEY = "your_ionet_api_key_here")
  
  # Basic usage - automatically runs on ALL available models
  result <- multiLLMviaionet(
    prompt = "Explain quantum computing in simple terms"
  )
  
  # Limit to first 5 models in order
  result <- multiLLMviaionet(
    prompt = "What is machine learning?",
    max_models = 5
  )
  
  # Random selection of 10 models from all available
  result <- multiLLMviaionet(
    prompt = "Write a Python function to calculate fibonacci numbers",
    max_models = 10,
    random_selection = TRUE,
    temperature = 0.3,
    streaming = FALSE
  )
  
  
  # Access results
  print(result$summary)
  lapply(result$results, function(x) cat(x$model, ":\n", x$response, "\n\n"))
}
}
\author{
Satoshi Kume
}
