% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiLLMviaionet.R
\name{multiLLMviaionet}
\alias{multiLLMviaionet}
\title{multiLLMviaionet: Execute multiple LLM models simultaneously via io.net API}
\usage{
multiLLMviaionet(
  prompt,
  models = c("meta-llama/Llama-3.3-70B-Instruct", "deepseek-ai/DeepSeek-R1",
    "Qwen/Qwen3-235B-A22B-FP8"),
  max_models = 6,
  streaming = FALSE,
  random_selection = FALSE,
  api_key = Sys.getenv("IONET_API_KEY"),
  max_tokens = 1024,
  temperature = 0.7,
  timeout = 300,
  parallel = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{prompt}{A string containing the input prompt to send to all selected models.}

\item{models}{A character vector of model names to use. Default includes 3 diverse models.
See details for complete list of available models.}

\item{max_models}{Integer specifying maximum number of models to run (1-10). Default is 6.}

\item{streaming}{Logical indicating whether to use streaming responses. Default is FALSE.}

\item{random_selection}{Logical indicating whether to randomly select models. 
If TRUE, ignores the 'models' parameter and randomly selects from all available models.
If FALSE, uses the specified models in order. Default is FALSE.}

\item{api_key}{A string containing the io.net API key. 
Defaults to the environment variable "IONET_API_KEY".}

\item{max_tokens}{Integer specifying maximum tokens to generate per model. Default is 1024.}

\item{temperature}{Numeric value controlling randomness (0-2). Default is 0.7.}

\item{timeout}{Numeric value in seconds for request timeout per model. Default is 300.}

\item{parallel}{Logical indicating whether to run models in parallel. Default is TRUE.}

\item{verbose}{Logical indicating whether to show detailed progress. Default is TRUE.}
}
\value{
A list containing:
  \describe{
    \item{results}{List of responses from each model with metadata}
    \item{summary}{Summary statistics including execution times and token usage}
    \item{errors}{Any errors encountered during execution}
    \item{models_used}{Character vector of models that were actually executed}
  }
}
\description{
This function allows you to run the same prompt across multiple LLM models 
  simultaneously using the io.net API. It supports up to 6 models running in parallel,
  with streaming responses and comprehensive error handling.
  
  Supported model categories include:
  - Meta Llama series (Llama-4-Maverick, Llama-3.3-70B, etc.)
  - DeepSeek series (DeepSeek-R1, DeepSeek-R1-Distill variants)
  - Qwen series (Qwen3-235B, QwQ-32B, Qwen2.5-Coder, etc.)
  - Specialized models (AceMath-7B, watt-tool-70B, ReaderLM-v2)
  - Microsoft Phi, Mistral, Google Gemma, and more
}
\details{
Multi-LLM via io.net API
}
\examples{
\dontrun{
  # Set your io.net API key
  Sys.setenv(IONET_API_KEY = "your_ionet_api_key_here")
  
  # Basic usage with default models
  result <- multiLLMviaionet(
    prompt = "Explain quantum computing in simple terms",
    models = c("meta-llama/Llama-3.3-70B-Instruct", 
               "deepseek-ai/DeepSeek-R1",
               "Qwen/Qwen3-235B-A22B-FP8")
  )
  
  # Advanced usage with random selection (10 models)
  result <- multiLLMviaionet(
    prompt = "Write a Python function to calculate fibonacci numbers",
    max_models = 10,
    random_selection = TRUE,
    temperature = 0.3,
    streaming = FALSE
  )
  
  # Quick random 10 model comparison
  result <- multiLLM_random10(
    prompt = "Explain machine learning in simple terms"
  )
  
  # Access results
  print(result$summary)
  lapply(result$results, function(x) cat(x$model, ":\n", x$response, "\n\n"))
}
}
\author{
Satoshi Kume
}
